[
  {
    "location": "src/lightning/fabric/utilities/init.py::_materialize_distributed_module",
    "code_snippet": "# TODO: Introduce `Fabric.materialize(module)` to give user control when materialization should happen\n# TODO: Make `torchmetrics.Metric` compatible with the `to_empty()` + `reset_parameters()` semantics",
    "suggested_feature": "Explicit materialization API for meta-initialized modules",
    "description": "Lightning currently auto-materializes meta-device modules during distributed setup, but the TODO notes a missing user-facing control point. A `Fabric.materialize(module)` API plus metric-compatible materialization would give users deterministic control over when initialization memory spikes happen, and reduce surprises when integrating custom modules/metrics in FSDP-style flows."
  },
  {
    "location": "src/lightning/fabric/plugins/precision/bitsandbytes.py::BitsandbytesPrecision.module_init_context",
    "code_snippet": "# TODO: this could also support replacing `Embedding` and `Conv1D`",
    "suggested_feature": "Quantization coverage beyond Linear layers",
    "description": "The bitsandbytes integration currently patches `torch.nn.Linear` only, with an explicit TODO for `Embedding` and `Conv1D`. Extending quantization to these modules would unlock larger memory savings and broader model compatibility (e.g., language and sequence models that rely heavily on embeddings), reducing manual user workarounds."
  },
  {
    "location": "src/lightning/fabric/plugins/precision/amp.py::MixedPrecision.unscale_gradients",
    "code_snippet": "raise NotImplementedError(\"Gradient clipping is not implemented for optimizers handling the unscaling.\")",
    "suggested_feature": "Unified AMP gradient clipping support",
    "description": "For optimizers that internally handle AMP unscaling, Lightning raises `NotImplementedError` for gradient clipping. Providing a backend-agnostic clipping path (or optimizer capability interface) would improve out-of-the-box stability for advanced optimizers and prevent runtime failures in mixed-precision training."
  },
  {
    "location": "src/lightning/fabric/utilities/throughput.py::_plugin_to_compute_dtype",
    "code_snippet": "# TODO: integrate this into the precision plugins",
    "suggested_feature": "Precision-plugin self-reporting for throughput dtype",
    "description": "Throughput measurement currently hard-codes dtype inference via plugin type checks. Moving this into a precision-plugin interface would make throughput tooling extensible for custom plugins and reduce maintenance when new precision backends are added."
  },
  {
    "location": "src/lightning/pytorch/strategies/launchers/multiprocessing.py::_recover_results_in_main_process",
    "code_snippet": "# TODO: pass also best score",
    "suggested_feature": "Complete checkpoint metadata recovery in multiprocessing launcher",
    "description": "When transferring training results back to the main process, Lightning restores `best_model_path` but not `best score`. Including full checkpoint callback metadata would make post-fit reporting and callback-driven decisions consistent across launchers."
  },
  {
    "location": "src/lightning/pytorch/callbacks/progress/rich_progress.py::on_validation_batch_start",
    "code_snippet": "# TODO: remove old tasks when new once they are created",
    "suggested_feature": "Task lifecycle management for rich validation progress bars",
    "description": "Validation progress tasks accumulate and are only hidden, not removed. Implementing task cleanup/recycling would reduce UI clutter and long-run overhead in multi-dataloader or long training sessions, improving user-facing progress readability."
  },
  {
    "location": "src/lightning/pytorch/core/optimizer.py::_configure_schedulers_automatic_opt + GitHub issue #21491",
    "code_snippet": "if \"interval\" in scheduler and scheduler[\"interval\"] not in (\"step\", \"epoch\"):\n    raise MisconfigurationException(...)\n\nIssue idea: add `interval: \"batch\"` for scheduler stepping with adaptive gradient accumulation.",
    "suggested_feature": "Batch-level scheduler interval",
    "description": "Scheduler interval validation allows only `step`/`epoch`, while users requested a `batch` interval to decouple schedules from optimizer steps when accumulation changes dynamically. Supporting `batch` would improve reproducibility and curriculum-style training where progress should follow batches/tokens seen."
  },
  {
    "location": "src/lightning/pytorch/loops/fit_loop.py::_should_reload_train_dl + GitHub issue #21448",
    "code_snippet": "n_epochs = self.trainer.reload_dataloaders_every_n_epochs\nreturn n_epochs and self.trainer.current_epoch - self._last_train_dl_reload_epoch >= n_epochs\n\nIssue idea: trigger in-place dataloader reloads based on runtime metrics.",
    "suggested_feature": "Metric-triggered dataloader reload hook",
    "description": "Dataloader reloading is epoch-count-based only. A callback/hook API for metric-triggered reloads would support curriculum learning and adaptive data sampling without restarting training or embedding complex control logic into datasets."
  },
  {
    "location": "src/lightning/fabric/plugins/environments/slurm.py::SLURMEnvironment.__init__ + GitHub issue #21404",
    "code_snippet": "if requeue_signal is None and not _IS_WINDOWS:\n    requeue_signal = signal.SIGUSR1\n\nIssue idea: make the default auto-requeue signal externally configurable.",
    "suggested_feature": "Configurable global default SLURM requeue signal",
    "description": "The SLURM plugin hardcodes `SIGUSR1` when no signal is passed. Exposing this as a class-level or environment-configurable default would let users adapt to cluster policies without custom environment-detection boilerplate."
  },
  {
    "location": "src/lightning/pytorch/trainer/connectors/checkpoint_connector.py::CheckpointConnector.dump_checkpoint + GitHub issue #21170",
    "code_snippet": "checkpoint = { ..., 'state_dict': ..., 'loops': ... }\nif not weights_only:\n    checkpoint['optimizer_states'] = ...\n    checkpoint['lr_schedulers'] = ...\n\nIssue idea: split model weights and training state into separate checkpoint files.",
    "suggested_feature": "Split checkpoint artifacts (weights vs trainer state)",
    "description": "Lightning writes one composite checkpoint dict (with `weights_only` as a coarse option). A first-class two-file format (inference artifact + resumable training state) would improve portability, artifact size management, and deployment workflows where optimizer/trainer state is unnecessary."
  },
  {
    "location": "src/lightning/pytorch/loggers/__init__.py + GitHub issue #21466",
    "code_snippet": "__all__ = ['LitLogger', 'CometLogger', 'CSVLogger', 'Logger', 'MLFlowLogger', 'TensorBoardLogger', 'WandbLogger', 'NeptuneLogger']\n\nIssue idea: add a dedicated `TrackioLogger`.",
    "suggested_feature": "Pluggable logger registry and new Trackio logger",
    "description": "Logger support currently appears as explicitly maintained integrations. A plugin/entry-point logger registry (plus Trackio implementation) would reduce maintenance friction, broaden ecosystem compatibility, and let users adopt lightweight experiment trackers without forking core Lightning loggers."
  }
]
